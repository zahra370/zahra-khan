<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MS Thesis | Zahra Khan | Robotics & AI Engineer</title>
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700;800;900&display=swap" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <style>
    :root {
      --primary-heading: #2d2d5a;
      --secondary-text: #5a5a7d;
      --accent-purple: #6a00ff;
      --light-bg: #f5f5f7;
      --white: #ffffff;
      --link-hover: #4e00b0;
      --icon-color: #6a00ff;
      --card-shadow: 0 10px 30px rgba(0,0,0,0.08);
    }

    body {
      font-family: 'Montserrat', sans-serif;
      background-color: var(--light-bg);
      color: var(--secondary-text);
      line-height: 1.6;
    }

    /* Navbar */
    .navbar {
      background-color: var(--white) !important;
      box-shadow: 0 2px 10px rgba(0,0,0,0.05);
      padding: 1rem 0;
      position: sticky;
      top: 0;
      z-index: 1000;
    }
    .navbar-brand {
      font-weight: 800;
      font-size: 1.8rem;
      color: var(--primary-heading) !important;
    }
    .nav-link {
      font-weight: 600;
      color: var(--secondary-text) !important;
      margin: 0 15px;
      transition: color 0.3s ease;
    }
    .nav-link:hover {
      color: var(--accent-purple) !important;
    }
    .nav-link.active {
      color: var(--accent-purple) !important;
    }

    /* Page Hero Section */
    .page-hero-section {
      background-color: var(--light-bg);
      padding: 80px 0 40px;
      text-align: center;
    }
    .page-hero-content h1 {
      font-size: 3.5rem;
      font-weight: 900;
      color: var(--primary-heading);
      margin-bottom: 15px;
      line-height: 1.1;
    }
    .page-hero-content p {
      font-size: 1.2rem;
      max-width: 700px;
      margin: 0 auto 30px;
      color: var(--secondary-text);
    }

    /* Sections */
    .section-title {
      font-size: 2.8rem;
      font-weight: 800;
      color: var(--primary-heading);
      text-align: center;
      margin-bottom: 60px;
      padding-top: 60px;
    }
    .bg-white-section {
      background-color: var(--white);
      padding: 80px 0;
      box-shadow: 0 0 20px rgba(0,0,0,0.03);
      margin-bottom: 30px;
    }

    /* Thesis Section */
    .thesis-section {
      background-color: var(--light-bg);
      padding: 80px 0;
    }
    .thesis-card {
      background-color: var(--white);
      border-radius: 15px;
      box-shadow: var(--card-shadow);
      padding: 40px;
      margin-bottom: 30px;
      transition: all 0.3s ease;
      height: 100%;
    }
    .thesis-card:hover {
      transform: translateY(-5px);
      box-shadow: 0 15px 30px rgba(0,0,0,0.12);
    }
    .thesis-card h3 {
      font-weight: 700;
      color: var(--primary-heading);
      margin-bottom: 20px;
      font-size: 1.5rem;
    }
    .thesis-card h4 {
      font-weight: 600;
      color: var(--accent-purple);
      margin-bottom: 15px;
      font-size: 1.2rem;
    }
    .thesis-card p {
      margin-bottom: 25px;
    }
    .btn-container {
      display: flex;
      gap: 15px;
      margin-top: 25px;
    }
    .pdf-preview {
      display: inline-block;
      background-color: var(--white);
      border: 2px solid var(--accent-purple);
      color: var(--accent-purple);
      padding: 12px 25px;
      border-radius: 8px;
      font-weight: 600;
      text-decoration: none;
      transition: all 0.3s ease;
      flex: 1;
      text-align: center;
    }
    .pdf-preview:hover {
      background-color: rgba(106, 0, 255, 0.05);
      color: var(--accent-purple);
      transform: translateY(-2px);
    }
    .pdf-download {
      display: inline-block;
      background-color: var(--accent-purple);
      color: var(--white);
      padding: 12px 25px;
      border-radius: 8px;
      font-weight: 600;
      text-decoration: none;
      transition: all 0.3s ease;
      flex: 1;
      text-align: center;
    }
    .pdf-download:hover {
      background-color: var(--link-hover);
      color: var(--white);
      transform: translateY(-2px);
    }
    .keywords {
      margin-top: 20px;
      font-style: italic;
      color: var(--secondary-text);
      background-color: rgba(106, 0, 255, 0.05);
      padding: 15px;
      border-radius: 8px;
    }
    .abstract-section {
      max-height: 300px;
      overflow-y: auto;
      padding-right: 10px;
      margin-bottom: 20px;
    }
    .abstract-section::-webkit-scrollbar {
      width: 6px;
    }
    .abstract-section::-webkit-scrollbar-track {
      background: rgba(106, 0, 255, 0.05);
      border-radius: 10px;
    }
    .abstract-section::-webkit-scrollbar-thumb {
      background: var(--accent-purple);
      border-radius: 10px;
    }

    /* Methodology Diagram Styling */
    .methodology-container {
      background-color: var(--white);
      border-radius: 15px;
      box-shadow: var(--card-shadow);
      padding: 40px;
      margin-bottom: 40px;
      text-align: center;
    }
    .methodology-diagram {
      width: 100%;
      max-width: 800px;
      border-radius: 10px;
      box-shadow: 0 5px 15px rgba(0,0,0,0.1);
      transition: transform 0.3s ease;
    }
    .methodology-diagram:hover {
      transform: scale(1.02);
    }
    .methodology-caption {
      text-align: center;
      margin-top: 15px;
      font-style: italic;
      color: var(--secondary-text);
      font-size: 0.9rem;
    }

    /* Footer */
    footer {
      background-color: var(--primary-heading);
      color: var(--light-bg);
      padding: 30px 0;
      text-align: center;
      font-size: 0.9rem;
    }
    footer a {
      color: var(--accent-purple);
      text-decoration: none;
    }
    footer a:hover {
      text-decoration: underline;
    }

    /* Responsive */
    @media (max-width: 991px) {
      .btn-container {
        flex-direction: column;
      }
    }
    @media (max-width: 576px) {
      .page-hero-content h1 {
        font-size: 2.5rem;
      }
      .section-title {
        font-size: 2.2rem;
      }
    }
  </style>
</head>
<body>

  <!-- Navbar -->
  <nav class="navbar navbar-expand-lg">
    <div class="container">
      <a class="navbar-brand" href="index.html">Zahra Khan</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse justify-content-end" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item"><a class="nav-link" href="index.html#about">About</a></li>
          <li class="nav-item"><a class="nav-link" href="index.html#research">Research</a></li>
          <li class="nav-item"><a class="nav-link active" href="ms-thesis.html">MS Thesis</a></li>
          <li class="nav-item"><a class="nav-link" href="projects.html">Projects</a></li>
          <li class="nav-item"><a class="nav-link" href="certifications.html">Certifications</a></li>
          <li class="nav-item"><a class="nav-link" href="cv.html" target="_blank">CV</a></li>
          <li class="nav-item"><a class="nav-link" href="index.html#contact">Contact</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Page Hero -->
  <section class="page-hero-section">
    <div class="container">
      <div class="page-hero-content">
        <h1>MS Thesis & Publications</h1>
        <p>My research in motion planning for nonholonomic differential drive robots using Deep Reinforcement Learning</p>
      </div>
    </div>
  </section>

  <!-- Research Overview -->
  <section class="bg-white-section">
    <div class="container">
      <h2 class="section-title">Research Overview</h2>
      <div class="row align-items-center">
        <div class="col-lg-8 col-md-12">
          <p class="thesis-title" style="font-weight: 800; color: #7c00ff; font-size: 1.2rem; margin-bottom: 20px;">
            Thesis Title: Motion Planning of Nonholonomic Differential Drive Robot in Cluttered Environment using Deep Reinforcement Learning
          </p>
          <p>
            My MS research focused on <b>motion planning for nonholonomic differential drive robots</b> in 
            complex environments using <b>Deep Reinforcement Learning (DRL)</b>. I designed a 
            <b>dense, SLAM-based reward function</b> to improve navigation, obstacle avoidance, and recovery 
            from local minima, major limitations in standard DRL methods.
          </p>
          <p>
            I implemented and compared three advanced DRL algorithms: 
            <b>Soft Actor-Critic (SAC)</b>, <b>Twin Delayed Deep Deterministic Policy Gradient (TD3)</b>, 
            and <b>Deep Deterministic Policy Gradient (DDPG)</b>. The algorithms were evaluated for 
            <b>convergence stability, smoothness, and goal-reaching accuracy</b>.
          </p>
          <p>
            The results demonstrated significant improvement in <b>trajectory smoothness, path efficiency</b>, 
            and <b>goal success rate</b>, showing that integrating <b>SLAM awareness into DRL</b> enhances 
            navigation in cluttered, real-world environments.
          </p>
        </div>
        <div class="col-lg-4 col-md-12 text-center">
          <div class="methodology-container">
            <img src="images/methodology-block-diagram-v2.jpg" alt="Methodology Block Diagram" class="methodology-diagram">
            <p class="methodology-caption">DRL-based Motion Planning Methodology</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- MS Thesis Section -->
  <section class="thesis-section">
    <div class="container">
      <h2 class="section-title">Thesis & Publications</h2>
      <div class="row">
        <div class="col-lg-6 mb-4">
          <div class="thesis-card">
            <h3>MS Thesis</h3>
            <h4>Robot Motion Planning using Deep Reinforcement Learning</h4>
            <div class="abstract-section">
              <p><strong>Abstract:</strong> Effective motion planning is needed for autonomous robots in challenging environments. 
              Sampling-based algorithms sample random points in high-dimensional spaces but struggle 
              to work well in complex environments due to slow convergence and inefficiencies. 
              Improvements in Deep Reinforcement Learning (DRL) overcome this strategy through 
              learning optimal policies from acting in the environment, reducing reliance on 
              environmental data and faster rates of convergence. But DRL resorts to sparse reward 
              functions to produce suboptimal paths and poor exploration. To advance beyond such 
              shortcomings, we propose an active SLAM-sourced information reward function. SLAM
              weighted reward enhances navigation efficiency with richer environment perception and 
              robustness in unexplored areas. It comprises distance-to-target and smoothness terms over 
              trajectory that encourage reduced distances, reduced oscillation, and more stable robot 
              performance. We use the Soft Actor-Critic (SAC) algorithm in our reward function, 
              because it is best to perform well in new environments. Comparison experiments using 
              Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy 
              Gradient (DDPG) in cluttered and sparse environments demonstrated the superior 
              performance of SAC. In sparse settings, SAC was 100% successful, performing 9.4% 
              better than TD3 and 14.1% better than DDPG, and using 35% fewer steps than TD3 and 
              63% fewer than DDPG. In chaotic settings, SAC was 87.5% successful, performing 40.6% 
              better than TD3 and 71.9% better than DDPG. These performances attest to the 
              unprecedented effectiveness of the SAC algorithm in the direction of autonomous robots.</p>
            </div>
            <div class="keywords">
              <strong>Keywords:</strong> Deep Reinforcement Learning, Differential drive robot, Model Free, 
              DDPG, TD3, SAC, ROS1, Gazebo
            </div>
            <div class="btn-container">
              <a href="zahra-thesis-final-23-07-2025.pdf" class="pdf-preview" target="_blank">Preview Thesis</a>
              <a href="zahra-thesis-final-23-07-2025.pdf" class="pdf-download" download>Download Thesis</a>
            </div>
          </div>
        </div>
        <div class="col-lg-6 mb-4">
          <div class="thesis-card">
            <h3>Research Paper</h3>
            <h4>A Map-Informed Dense Reward Function for Efficient Robot Motion Planning Using Deep Reinforcement Learning</h4>
            <div class="abstract-section">
              <p><strong>Abstract:</strong> Developing an effective motion planning algorithm is essential for autonomous robots to navigate challenging environments. Sampling-based algorithms explore high dimensional spaces efficiently through random sampling. However, they face challenges in the navigation of complex and cluttered environments due to random sampling and slow convergence. Recent progress in Deep Reinforcement Learning (DRL) mitigates these challenges by learning optimal policies through interaction with the environment. This enables autonomous agents to achieve faster convergence, adapt to complex environments, and navigate more effectively. However, relying on the episodic and sparse reward function leads to inefficient exploration and generates suboptimal paths. To address this issue, this research proposes a reward function that integrates map-based information. This reward improves environmental perception, enhances navigation efficiency, and generalizes to unseen environments. Moreover, the distance and trajectory smoothness rewards generate shorter paths and reduce oscillations in robot movement. Our proposed dense reward function is tested and compared with other reward functions from the literature, as well as with DRL algorithms such as Soft Actor Critic (SAC), Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG) in both sparse and cluttered environments. Experimental results demonstrate that SAC achieved superior performance. In sparse environment SAC achieved 100% success rate, outperforming TD3 at 98.44% and DDPG at 85.94%. In cluttered environments SAC achieved 87.5% success rate, significantly surpassing TD3 at 46.9% and DDPG at 15.6%. These results highlight SAC greater efficiency in navigating unseen environments, and its ability to generalize better compared to other algorithms.</p>
            </div>
            <div class="btn-container">
              <a href="Robot_Motion_Planning_using_Deep_Reinforcement_Learning_Zahra-khan.pdf" class="pdf-preview" target="_blank">Preview Paper</a>
              <a href="Robot_Motion_Planning_using_Deep_Reinforcement_Learning_Zahra-khan.pdf" class="pdf-download" download>Download Paper</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer>
    <div class="container">
      <p>&copy; 2025 Zahra Khan. All Rights Reserved. | <a href="#">Privacy Policy</a> | <a href="#">Terms of Service</a></p>
    </div>
  </footer>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>